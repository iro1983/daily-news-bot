import os
import feedparser
import google.generativeai as genai
from datetime import datetime
import time

# ================= é…ç½®åŒº =================
api_key = os.environ.get("GOOGLE_API_KEY")

# ================= 1. æ™ºèƒ½æ¨¡å‹é€‰æ‹© =================
def get_best_model():
    if not api_key: return None
    genai.configure(api_key=api_key)
    try:
        # å¿…é¡»ç”¨ Flashï¼Œå› ä¸ºè¿™æ¬¡æ•°æ®é‡æ›´å¤§ï¼ŒFlash çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›æœ€é€‚åˆ
        all_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        for m in all_models:
            if 'flash' in m and '1.5' in m: return genai.GenerativeModel(m)
        return genai.GenerativeModel('gemini-1.5-flash')
    except: return None

# ================= 2. æ·±åº¦æ•°æ®é‡‡é›† (åŠ å¤§é‡‡é›†é‡) =================
def get_data():
    print("ğŸš€ å¼€å§‹å…¨ç½‘æƒ…æŠ¥æŒ–æ˜ (åŠ é‡ç‰ˆ)...")
    data_text = ""
    
    # 1. Google Trends (å®è§‚) -> å¢åŠ åˆ° 20 æ¡
    try:
        feed = feedparser.parse("https://trends.google.com/trends/trendingsearches/daily/rss?geo=US")
        data_text += "\nã€Google Trends (Macro Traffic)ã€‘:\n"
        for entry in feed.entries[:20]: # æŠ“å–å‰ 20 ä¸ª
            traffic = getattr(entry, 'ht_approx_traffic', 'N/A')
            data_text += f"- Keyword: {entry.title} (Traffic: {traffic})\n  News: {entry.description}\n"
    except Exception as e:
        print(f"âš ï¸ Google è·³è¿‡: {e}")

    # 2. Reddit å‚ç›´æ¿å— (å¾®è§‚ç—›ç‚¹) -> æ¯ä¸ªæ¿å—å¢åŠ åˆ° 8 æ¡
    reddit_feeds = [
        ("r/Entrepreneur", "https://www.reddit.com/r/Entrepreneur/top/.rss?t=day"), 
        ("r/SideProject", "https://www.reddit.com/r/SideProject/top/.rss?t=day"),   
        ("r/technology", "https://www.reddit.com/r/technology/top/.rss?t=day"),     
        ("r/popular", "https://www.reddit.com/r/popular/top/.rss?t=day"),           
        ("r/marketing", "https://www.reddit.com/r/marketing/top/.rss?t=day")        
    ]

    for source_name, url in reddit_feeds:
        try:
            print(f"ğŸ’ æ­£åœ¨æŒ–æ˜ {source_name}...")
            feed = feedparser.parse(url, agent="Mozilla/5.0")
            if feed.entries:
                data_text += f"\nã€Source: {source_name}ã€‘:\n"
                # å¢åŠ æŠ“å–é‡ï¼šæ¯ä¸ªæ¿å—æŠ“ 8 æ¡ï¼Œ5ä¸ªæ¿å—å°±æ˜¯ 40 æ¡
                for entry in feed.entries[:8]: 
                    content_snippet = "No Content"
                    if hasattr(entry, 'content'):
                        content_snippet = entry.content[0].value[:600]
                    elif hasattr(entry, 'summary'):
                        content_snippet = entry.summary[:600]
                    content_snippet = content_snippet.replace("<p>", "").replace("</p>", "").replace("<br>", " ")
                    
                    data_text += f"--- Post ---\nTitle: {entry.title}\nLink: {entry.link}\nSnippet: {content_snippet}\n"
        except Exception as e:
            print(f"âš ï¸ {source_name} è·³è¿‡")
            
    return data_text

# ================= 3. AI åŒè¯­å•†ä¸šåˆ†æå¸ˆ (å¼ºåˆ¶ 15+ æ¡) =================
def analyze_to_html(text_data):
    model = get_best_model()
    if not model: return "<h1>AI é…ç½®å¤±è´¥</h1>"

    date_str = datetime.now().strftime("%Y-%m-%d")
    print(f"ğŸ§  AI æ­£åœ¨è¿›è¡Œå¤§è§„æ¨¡åŒè¯­åˆ†æ...")
    
    prompt = f"""
    You are an expert "Business Intelligence Analyst". Today is {date_str}.
    I have provided extensive raw data from Google Trends and Reddit.

    ã€Goalã€‘
    Create a **Bilingual (English & Chinese)** Business Intelligence Report.
    Identify **Business Opportunities**, **User Pain Points**, and **Viral Trends**.

    ã€Quantity Requirementã€‘
    **CRITICAL**: You MUST generate at least **15 to 20 items** in total. Do not output less than 15 items.

    ã€Output Structureã€‘
    Please generate 3 sections. Each section must have 5-6 items.

    ### Section 1: ğŸš€ Business Opportunities & Pain Points (å•†æœºä¸ç—›ç‚¹)
    *Target: 5-6 items* (Source: r/Entrepreneur, r/SideProject, r/marketing)
    * **Card Content**:
      - **Headline**: English Title / ä¸­æ–‡æ ‡é¢˜
      - **Analysis (Bilingual)**:
        - **EN**: Briefly analyze the pain point or opportunity.
        - **CN**: ç®€è¦åˆ†æç—›ç‚¹æˆ–å•†æœºã€‚
      - **Actionable Tip**: One specific advice (Bilingual).

    ### Section 2: ğŸ”¥ Viral Trends & Traffic (æµé‡å¯†ç )
    *Target: 5-6 items* (Source: Google Trends & r/popular)
    * **Card Content**:
      - **Headline**: English Keyword / ä¸­æ–‡çƒ­è¯
      - **Context (Bilingual)**:
        - **EN**: Why is this trending?
        - **CN**: ä¸ºä»€ä¹ˆç«äº†ï¼Ÿ
      - **Marketing Angle**: How to use this trend? (Bilingual).

    ### Section 3: ğŸ’¡ Tech & Industry Signals (è¡Œä¸šä¿¡å·)
    *Target: 5-6 items* (Source: r/technology, r/business)
    * **Card Content**:
      - **Headline**: English Event / ä¸­æ–‡äº‹ä»¶
      - **Impact (Bilingual)**:
        - **EN**: Why does it matter?
        - **CN**: ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

    ã€Design & CSSã€‘
    - **Theme**: Dark Professional Mode (#1a1b1e background).
    - **Typography**: English (Light Gray #ced4da), Chinese (White #ffffff).
    - **Layout**: Grid cards (Responsive).
    - **Tags**: Show Source & Category tags clearly.

    ã€Raw Dataã€‘
    {text_data}
    
    Output ONLY valid HTML code.
    """

    try:
        response = model.generate_content(prompt)
        return response.text.replace("```html", "").replace("```", "")
    except Exception as e:
        return f"<h1>ç”Ÿæˆå‡ºé”™</h1><p>{e}</p>"

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    raw_data = get_data()
    if not raw_data: raw_data = "No data."
    
    html_page = analyze_to_html(raw_data)
    
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html_page)
    
    print("âœ… 15+æ¡åŒè¯­æƒ…æŠ¥ç”Ÿæˆå®Œæˆ")
