import os
import feedparser
import google.generativeai as genai
from pytrends.request import TrendReq
from datetime import datetime

# ================= é…ç½®åŒº =================
api_key = os.environ.get("GOOGLE_API_KEY")

# ================= 1. æ ¸å¿ƒï¼šæ™ºèƒ½å¯»æ‰¾å¯ç”¨çš„ Flash æ¨¡å‹ =================
def get_best_model():
    if not api_key:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° API Key")
        return None
    
    genai.configure(api_key=api_key)
    print("ğŸ” æ­£åœ¨æ‰«æå¯ç”¨æ¨¡å‹åˆ—è¡¨...")
    
    try:
        # è·å–æ‰€æœ‰æ”¯æŒç”Ÿæˆçš„æ¨¡å‹
        all_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        print(f"ğŸ“‹ ä½ çš„è´¦å·æ”¯æŒè¿™äº›æ¨¡å‹: {all_models}")
        
        # ç­–ç•¥ 1: ä¼˜å…ˆæ‰¾ç¨³å®šçš„ Flash (é€Ÿåº¦å¿«ï¼Œé¢åº¦é«˜)
        for m in all_models:
            if 'flash' in m and 'exp' not in m and '1.5' in m:
                print(f"âœ… é€‰ä¸­æœ€ä½³æ¨¡å‹: {m}")
                return genai.GenerativeModel(m)
        
        # ç­–ç•¥ 2: å¦‚æœæ²¡æœ‰æ ‡å‡† Flashï¼Œæ‰¾ä»»ä½•å¸¦ Flash çš„
        for m in all_models:
            if 'flash' in m:
                print(f"âœ… é€‰ä¸­å¤‡ç”¨ Flash æ¨¡å‹: {m}")
                return genai.GenerativeModel(m)

        # ç­–ç•¥ 3: æ‰¾ Pro
        for m in all_models:
            if 'pro' in m and 'exp' not in m:
                print(f"âœ… é€‰ä¸­ Pro æ¨¡å‹: {m}")
                return genai.GenerativeModel(m)

        # ç­–ç•¥ 4: éšä¾¿é€‰ä¸€ä¸ªèƒ½ç”¨çš„
        if all_models:
            print(f"âš ï¸ æ²¡æ‰¾åˆ°å¸¸ç”¨æ¨¡å‹ï¼Œå¼ºåˆ¶ä½¿ç”¨ç¬¬ä¸€ä¸ª: {all_models[0]}")
            return genai.GenerativeModel(all_models[0])
            
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return None
    
    print("âŒ ä¸¥é‡ï¼šæœªæ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹ï¼")
    return None

# ================= 2. æŠ“å–æ•°æ® =================
def get_data():
    print("ğŸš€ å¼€å§‹é‡‡é›†æ•°æ®...")
    data_text = ""
    
    # Reddit RSS
    try:
        print("æ­£åœ¨è¿æ¥ Reddit...")
        feed = feedparser.parse("https://www.reddit.com/r/popular/top/.rss?t=day", agent="Mozilla/5.0")
        if feed.entries:
            data_text += "ã€Reddit çƒ­é—¨è¯é¢˜ã€‘:\n"
            for entry in feed.entries[:7]:
                data_text += f"- {entry.title}\n"
    except Exception as e:
        print(f"âš ï¸ Reddit è·³è¿‡: {e}")

    # Google Trends (ç»å¸¸ 404ï¼Œä¸ç”¨ç®¡å®ƒï¼Œå±äºæ­£å¸¸ç°è±¡)
    try:
        print("æ­£åœ¨è¿æ¥ Google Trends...")
        pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25))
        trends = pytrends.trending_searches(pn='united_states').head(10)[0].tolist()
        data_text += "\nã€Google æœç´¢çƒ­è¯ã€‘:\n"
        for t in trends:
            data_text += f"- {t}\n"
    except Exception as e:
        print(f"âš ï¸ Google Trends è·³è¿‡ (äº‘æœåŠ¡å™¨å¸¸è¢«Googleæ‹¦æˆªï¼Œå±æ­£å¸¸): {e}")
            
    return data_text

# ================= 3. ç”Ÿæˆç½‘é¡µ =================
def analyze_to_html(text_data):
    # æ™ºèƒ½è·å–æ¨¡å‹
    model = get_best_model()
    if not model:
        return "<h1>AI é…ç½®å¤±è´¥</h1><p>æ— æ³•æ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ Logsã€‚</p>"

    date_str = datetime.now().strftime("%Y-%m-%d")
    print(f"ğŸ¤– å¼€å§‹ç”Ÿæˆå†…å®¹...")
    
    prompt = f"""
    ä»Šå¤©æ˜¯ {date_str}ã€‚
    è¯·æ ¹æ®ä»¥ä¸‹æ•°æ®å†™ä¸€ä¸ª HTML5 ç½‘é¡µã€‚
    
    è¦æ±‚ï¼š
    1. å¿…é¡»æ˜¯å®Œæ•´çš„ HTML ç»“æ„ï¼ŒåŒ…å« <head> å’Œ <body>ã€‚
    2. ä½¿ç”¨å†…åµŒ CSS ç¾åŒ–ï¼Œé£æ ¼ä¸ºâ€œæç®€æ–°é—»æ—¥æŠ¥â€ï¼ŒèƒŒæ™¯è‰² #f4f4f9ï¼Œå¡ç‰‡ç™½åº•åœ†è§’ã€‚
    3. æ ‡é¢˜ï¼šğŸ‡ºğŸ‡¸ ç¾å›½å…¨ç½‘çƒ­ç‚¹æ—¥æŠ¥ ({date_str})ã€‚
    4. å†…å®¹ï¼šé€‰å‡º 5 ä¸ªæœ€çƒ­äº‹ä»¶ï¼Œæ¯ä¸ªäº‹ä»¶ä¸€ä¸ªå¡ç‰‡ã€‚
    5. åªè¦è¾“å‡º HTML ä»£ç ã€‚
    
    æ•°æ®ï¼š
    {text_data}
    """

    try:
        response = model.generate_content(prompt)
        return response.text.replace("```html", "").replace("```", "")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå‡ºé”™: {e}")
        return f"<h1>ç”Ÿæˆå‡ºé”™</h1><p>{e}</p>"

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    raw_data = get_data()
    if not raw_data:
        raw_data = "æš‚æ— æ•°æ®ã€‚"
    
    html_page = analyze_to_html(raw_data)
    
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html_page)
    
    print("âœ… ä»»åŠ¡å®Œæˆ")
