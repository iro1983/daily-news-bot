import os
import feedparser
import google.generativeai as genai
from datetime import datetime

# ================= é…ç½®åŒº =================
api_key = os.environ.get("GOOGLE_API_KEY")

# ================= 1. æ™ºèƒ½æ¨¡å‹é€‰æ‹© =================
def get_best_model():
    if not api_key:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° API Key")
        return None
    genai.configure(api_key=api_key)
    try:
        all_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        # ä¼˜å…ˆæ‰¾ Flash (é€Ÿåº¦å¿«ï¼Œé•¿æ–‡æœ¬èƒ½åŠ›å¼ºï¼Œé€‚åˆåŒè¯­è¾“å‡º)
        for m in all_models:
            if 'flash' in m and 'exp' not in m and '1.5' in m: return genai.GenerativeModel(m)
        for m in all_models:
            if 'flash' in m: return genai.GenerativeModel(m)
        if all_models: return genai.GenerativeModel(all_models[0])
    except: return None
    return None

# ================= 2. å¼ºåŠ›æ•°æ®é‡‡é›† (Google RSS + Reddit) =================
def get_data():
    print("ğŸš€ å¼€å§‹å…¨ç½‘çƒ­ç‚¹é‡‡é›†...")
    data_text = ""
    
    # 1. Google Trends RSS (æç¨³)
    try:
        print("ğŸ”¥ æ­£åœ¨æŠ“å– Google å®æ—¶çƒ­æœ...")
        feed = feedparser.parse("https://trends.google.com/trends/trendingsearches/daily/rss?geo=US")
        if feed.entries:
            data_text += "\nã€Source: Google Trends USã€‘:\n"
            for entry in feed.entries[:20]: # æŠ“å‰20ä¸ª
                traffic = getattr(entry, 'ht_approx_traffic', 'N/A')
                data_text += f"- Keyword: {entry.title} (Traffic: {traffic})\n"
                data_text += f"  News Snippet: {entry.description}\n"
    except Exception as e:
        print(f"âš ï¸ Google RSS æŠ“å–å¼‚å¸¸: {e}")

    # 2. Reddit åƒç“œä¸çƒ­è®®
    reddit_feeds = [
        ("r/popular", "https://www.reddit.com/r/popular/top/.rss?t=day"),
        ("r/technology", "https://www.reddit.com/r/technology/top/.rss?t=day"),
        ("r/entertainment", "https://www.reddit.com/r/entertainment/top/.rss?t=day")
    ]

    for source_name, url in reddit_feeds:
        try:
            print(f"ğŸ’¬ æ­£åœ¨æŠ“å– {source_name}...")
            feed = feedparser.parse(url, agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
            if feed.entries:
                data_text += f"\nã€Source: {source_name}ã€‘:\n"
                for entry in feed.entries[:20]: 
                    data_text += f"- Title: {entry.title}\n  Link: {entry.link}\n"
        except Exception as e:
            print(f"âš ï¸ {source_name} æŠ“å–è·³è¿‡")
            
    return data_text

# ================= 3. AI åŒè¯­åˆ†æä¸ç½‘é¡µç”Ÿæˆ =================
def analyze_to_html(text_data):
    model = get_best_model()
    if not model: return "<h1>AI é…ç½®å¤±è´¥</h1>"

    date_str = datetime.now().strftime("%Y-%m-%d")
    print(f"ğŸ¤– AI æ­£åœ¨è¿›è¡Œä¸­è‹±æ–‡åŒè¯­ç¼–è¯‘...")
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šåŒè¯­ Prompt ---
    prompt = f"""
    You are the editor-in-chief of "Daily US Trends". Today is {date_str}.
    Based on the raw data from Google Trends and Reddit provided below, create a **Bilingual (English & Chinese)** HTML news report.

    ã€Content Strategyã€‘
    1. **Selection**: Pick top 20 most interesting/viral stories. Prioritize topics with high traffic on Google or intense discussion on Reddit.
    2. **Bilingual Requirement**: Every section MUST correspond in English and Chinese.
    
    ã€Card Structure (HTML)ã€‘
    For each story card, include:
    - **Header**: 
      - English Headline (Catchy)
      - ä¸­æ–‡æ ‡é¢˜ (ç¿»è¯‘è¦æ¥åœ°æ°”ï¼Œæœ‰å¸å¼•åŠ›)
    - **Meta Info**: Source (e.g., Google Trends) & Tags.
    - **The Story (Content)**:
      - A paragraph in **English** summarizing the event (approx 60-100 words).
      - A paragraph in **Chinese** summarizing the same event.
    - **Netizen Vibe (Reaction)**:
      - What are people saying? (One sentence EN / One sentence CN).

    ã€Design & CSS Requirementsã€‘
    - **Theme**: Dark Mode (#121212 background).
    - **Typography**: Clean sans-serif. 
    - **Contrast**: 
       - Make the **English text** a slightly lighter gray (e.g., #e0e0e0).
       - Make the **Chinese text** bright white (e.g., #ffffff) or a highlight color to distinguish them easily.
    - **Layout**: Grid layout (responsive). Card background: #1e1e1e. Border-radius: 12px.
    - **Styling**: Use distinct spacing or a subtle divider line between English and Chinese sections within the card.

    ã€Raw Dataã€‘
    {text_data}
    
    Output ONLY valid HTML code. Do not use Markdown blocks.
    """

    try:
        response = model.generate_content(prompt)
        return response.text.replace("```html", "").replace("```", "")
    except Exception as e:
        return f"<h1>ç”Ÿæˆå‡ºé”™</h1><p>{e}</p>"

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    raw_data = get_data()
    if not raw_data: raw_data = "No data available."
    
    html_page = analyze_to_html(raw_data)
    
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html_page)
    
    print("âœ… åŒè¯­æ—¥æŠ¥ç”Ÿæˆå®Œæˆ")
