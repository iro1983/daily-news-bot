import os
import json
import re
import feedparser
import google.generativeai as genai
from datetime import datetime

# ================= é…ç½®åŒº =================
api_key = os.environ.get("GOOGLE_API_KEY")

# ================= 1. åŠ¨æ€è·å–æ¨¡å‹ =================
def get_best_model():
    if not api_key: return None
    genai.configure(api_key=api_key)
    try:
        model_list = list(genai.list_models())
        supported_models = [m.name for m in model_list if 'generateContent' in m.supported_generation_methods]
        for name in supported_models:
            if 'flash' in name.lower(): return genai.GenerativeModel(name)
        for name in supported_models:
            if 'pro' in name.lower(): return genai.GenerativeModel(name)
        if supported_models: return genai.GenerativeModel(supported_models[0])
    except: return None
    return None

# ================= 2. æ·±åº¦æ•°æ®é‡‡é›† =================
def get_data():
    print("ğŸš€ å¼€å§‹å…¨ç½‘æƒ…æŠ¥æŒ–æ˜...")
    data_list = [] # è¿™ä¸€æ¬¡æˆ‘ä»¬ç”¨åˆ—è¡¨å­˜å‚¨ï¼Œæ–¹ä¾¿å¤„ç†
    
    # 1. Google Trends
    try:
        feed = feedparser.parse("https://trends.google.com/trends/trendingsearches/daily/rss?geo=US")
        print(f"ğŸ“Š Google Trends æŠ“å–åˆ° {len(feed.entries)} æ¡")
        for entry in feed.entries[:12]:
            traffic = getattr(entry, 'ht_approx_traffic', 'N/A')
            # æ„é€ åŸå§‹æ•°æ®å—
            data_list.append({
                "source": "Google Trends",
                "title": entry.title,
                "link": getattr(entry, 'link', '#'),
                "raw_content": f"Traffic: {traffic}\nNews Snippet: {entry.description}"
            })
    except Exception as e:
        print(f"âš ï¸ Google è·³è¿‡: {e}")

    # 2. Reddit
    reddit_feeds = [
        ("r/Entrepreneur", "https://www.reddit.com/r/Entrepreneur/top/.rss?t=day"), 
        ("r/SideProject", "https://www.reddit.com/r/SideProject/top/.rss?t=day"),   
        ("r/technology", "https://www.reddit.com/r/technology/top/.rss?t=day"),     
        ("r/popular", "https://www.reddit.com/r/popular/top/.rss?t=day"),           
    ]

    for source_name, url in reddit_feeds:
        try:
            print(f"ğŸ’ æ­£åœ¨æŒ–æ˜ {source_name}...")
            feed = feedparser.parse(url, agent="Mozilla/5.0")
            for entry in feed.entries[:6]: 
                # æå–æ­£æ–‡
                raw_txt = "No text content."
                if hasattr(entry, 'content'):
                    raw_txt = entry.content[0].value[:1500] # æŠ“æ›´å¤š
                elif hasattr(entry, 'summary'):
                    raw_txt = entry.summary[:1500]
                
                # æ¸…æ´— HTML æ ‡ç­¾ï¼Œåªç•™çº¯æ–‡æœ¬ï¼Œé˜²æ­¢ç ´å JSON ç»“æ„
                clean_txt = re.sub('<[^<]+?>', ' ', raw_txt)
                clean_txt = clean_txt.replace('"', "'").replace('\n', ' ') # æ›¿æ¢æ‰å¯èƒ½ç ´åJSONçš„å­—ç¬¦
                
                data_list.append({
                    "source": source_name,
                    "title": entry.title,
                    "link": entry.link,
                    "raw_content": clean_txt[:800] + "..." # æˆªæ–­ä¸€ä¸‹é˜²æ­¢è¿‡é•¿
                })
        except Exception as e:
            print(f"âš ï¸ {source_name} è·³è¿‡")
            
    return data_list

# ================= 3. AI åˆ†æ (åªè´Ÿè´£è¾“å‡º JSON) =================
def analyze_data(data_list):
    model = get_best_model()
    if not model: return []

    print(f"ğŸ§  AI æ­£åœ¨åˆ†æ {len(data_list)} æ¡æ•°æ®...")
    
    # å°†æ•°æ®åˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²å–‚ç»™ AI
    data_str = json.dumps(data_list, ensure_ascii=False, indent=1)
    
    prompt = f"""
    You are a Business Intelligence Analyst. 
    Analyze the following raw data list and identify **15 most valuable insights**.

    ã€Input Dataã€‘
    {data_str}

    ã€Taskã€‘
    Return a **JSON Array** of objects. Do not output Markdown or HTML. Just raw JSON.
    Each object in the array must have these exact keys:
    1. "category": (String) Choose one: "Business Opportunity", "Viral Trend", "Tech Signal".
    2. "source_tag": (String) e.g., "Google Trends", "r/Entrepreneur".
    3. "title_en": (String) Catchy English title.
    4. "title_cn": (String) Chinese title.
    5. "insight_en": (String) English analysis (Why it matters).
    6. "insight_cn": (String) Chinese analysis.
    7. "original_content": (String) **COPY verbatim** the "raw_content" from the input data corresponding to this item. Do not summarize it.

    ã€Format Exampleã€‘
    [
      {{
        "category": "Business Opportunity",
        "source_tag": "r/SideProject",
        "title_en": "AI Tool Demand",
        "title_cn": "AIå·¥å…·éœ€æ±‚",
        "insight_en": "Users are looking for...",
        "insight_cn": "ç”¨æˆ·æ­£åœ¨å¯»æ‰¾...",
        "original_content": "I built this app because..."
      }}
    ]
    """

    try:
        response = model.generate_content(prompt)
        # æ¸…æ´— AI è¿”å›çš„å¯èƒ½åŒ…å« ```json çš„æ ‡è®°
        clean_json = response.text.replace("```json", "").replace("```", "").strip()
        # è§£æ JSON
        analyzed_list = json.loads(clean_json)
        return analyzed_list
    except Exception as e:
        print(f"âŒ AI åˆ†æ JSON å¤±è´¥: {e}")
        # å¦‚æœå‡ºé”™ï¼Œæ‰“å°åŸå§‹å†…å®¹æ–¹ä¾¿è°ƒè¯•
        print(response.text[:500]) 
        return []

# ================= 4. Python æ¸²æŸ“ HTML (å¼ºåˆ¶ç”ŸæˆæŒ‰é’®) =================
def render_html(analyzed_data):
    date_str = datetime.now().strftime("%Y-%m-%d")
    
    # è¿™é‡Œçš„ CSS æ˜¯å†™æ­»çš„ï¼Œä¿è¯ä¸€å®šç¾è§‚
    css = """
    <style>
        :root { --bg: #0f172a; --card: #1e293b; --text: #e2e8f0; --accent: #38bdf8; }
        body { font-family: sans-serif; background: var(--bg); color: var(--text); padding: 20px; margin: 0; }
        .header { text-align: center; margin-bottom: 40px; }
        .header h1 { background: linear-gradient(90deg, #38bdf8, #818cf8); -webkit-background-clip: text; -webkit-text-fill-color: transparent; }
        
        /* ç€‘å¸ƒæµç½‘æ ¼ */
        .grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(360px, 1fr)); gap: 24px; max-width: 1400px; margin: 0 auto; }
        
        /* å¡ç‰‡ */
        .card { background: var(--card); padding: 24px; border-radius: 16px; border: 1px solid #334155; display: flex; flex-direction: column; }
        .tags { margin-bottom: 15px; }
        .tag { font-size: 12px; padding: 4px 8px; border-radius: 4px; background: #334155; margin-right: 5px; color: #94a3b8; }
        .tag.biz { color: #facc15; border: 1px solid #713f12; background: #422006; }
        .tag.trend { color: #f472b6; border: 1px solid #831843; background: #500724; }
        
        h2 { font-size: 18px; margin: 0 0 5px 0; color: #fff; }
        h3 { font-size: 14px; margin: 0 0 15px 0; color: #94a3b8; font-weight: normal; }
        
        .insight { background: #0f172a; padding: 15px; border-radius: 8px; margin-bottom: 15px; border-left: 3px solid var(--accent); }
        .insight p { margin: 5px 0; font-size: 14px; }
        .en { color: #cbd5e1; }
        .cn { color: #94a3b8; }
        
        /* å¼ºåˆ¶æ˜¾ç¤ºçš„åŸæ–‡åŒºåŸŸ */
        details { margin-top: auto; border-top: 1px solid #334155; padding-top: 10px; }
        summary { cursor: pointer; color: var(--accent); font-size: 14px; font-weight: bold; }
        .raw { background: #000; color: #4ade80; padding: 15px; border-radius: 6px; font-family: monospace; font-size: 12px; margin-top: 10px; white-space: pre-wrap; word-break: break-word; max-height: 250px; overflow-y: auto; }
    </style>
    """
    
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Daily Business Intelligence {date_str}</title>
        {css}
    </head>
    <body>
        <div class="header">
            <h1>ğŸ‡ºğŸ‡¸ Global Business Intelligence / å…¨çƒå•†ä¸šæƒ…æŠ¥</h1>
            <p>{date_str} â€¢ Powered by Gemini 1.5 Flash</p>
        </div>
        <div class="grid">
    """
    
    # Python å¾ªç¯ç”Ÿæˆå¡ç‰‡ (è¿™æ˜¯å…³é”®ï¼ç”±ä»£ç æ§åˆ¶ç»“æ„ï¼Œè€Œä¸æ˜¯ AI)
    for item in analyzed_data:
        # æ ¹æ®åˆ†ç±»ç»™ Tag æ¢é¢œè‰²
        tag_class = "tag"
        if "Business" in item.get('category', ''): tag_class = "tag biz"
        if "Viral" in item.get('category', ''): tag_class = "tag trend"
        
        html_content += f"""
        <div class="card">
            <div class="tags">
                <span class="{tag_class}">{item.get('category', 'General')}</span>
                <span class="tag">{item.get('source_tag', 'Web')}</span>
            </div>
            <h2>{item.get('title_en', 'No Title')}</h2>
            <h3>{item.get('title_cn', 'æ— æ ‡é¢˜')}</h3>
            
            <div class="insight">
                <p class="en">ğŸ’¡ {item.get('insight_en', 'No analysis')}</p>
                <p class="cn">ğŸ” {item.get('insight_cn', 'æ— åˆ†æ')}</p>
            </div>
            
            <details>
                <summary>â–¶ Click to View Raw Content / æŸ¥çœ‹åŸæ–‡è¯¦æƒ…</summary>
                <div class="raw">
{item.get('original_content', 'No content available.')}
                </div>
            </details>
        </div>
        """
        
    html_content += """
        </div>
    </body>
    </html>
    """
    return html_content

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    # 1. è·å–åŸå§‹æ•°æ®
    raw_data_list = get_data()
    
    if not raw_data_list:
        print("âŒ æ²¡æœ‰æŠ“å–åˆ°æ•°æ®")
        exit()
        
    # 2. AI åˆ†æ (è¿”å› JSON å¯¹è±¡)
    analyzed_data = analyze_data(raw_data_list)
    
    # 3. å¦‚æœ AI è§£æå¤±è´¥ï¼Œç”Ÿæˆä¸€ä¸ªç©ºçš„ HTML é˜²æ­¢æŠ¥é”™
    if not analyzed_data:
        print("âŒ AI è¿”å›çš„æ•°æ®æ ¼å¼æœ‰è¯¯")
        analyzed_data = []

    # 4. Python æ¸²æŸ“ HTML
    html_page = render_html(analyzed_data)
    
    # 5. ä¿å­˜
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html_page)
    
    print("âœ… å¼ºåˆ¶ç»“æ„åŒ–ç½‘é¡µç”Ÿæˆå®Œæˆ")
